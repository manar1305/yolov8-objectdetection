{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75984b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c188e8",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Affichage d'images et de vidéos \n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5686a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers la vidéo locale à afficher\n",
    "video_path = '/Users/macbook/Desktop/Project1/vehicle-counting.mp4'\n",
    "# Affichage de la vidéo avec une largeur et une hauteur spécifiées\n",
    "display(Video(video_path, embed=True, width=640, height=360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle YOLO\n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "# Obtention des noms de classes à partir du modèle\n",
    "dict_classes = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour redimensionner une image \n",
    "def resize_frame(frame, scale_percent):\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    resized = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fb9bb",
   "metadata": {},
   "source": [
    "# Detecting vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37342f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "verbose = False\n",
    "scale_percent = 50\n",
    "\n",
    "# Lecture de la vidéo avec OpenCV (cv2)\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Objets à détecter avec YOLO\n",
    "class_IDS = [2, 3, 5, 7] \n",
    "centers_old = {}\n",
    "centers_new = {}\n",
    "obj_id = 0 \n",
    "in_objects_counter = dict.fromkeys(class_IDS, 0)   # Compteur d'objets entrant par classe\n",
    "out_objects_counter = dict.fromkeys(class_IDS, 0)  # Compteur d'objets sortant par classe\n",
    "end = []\n",
    "frames_list = []  # Liste pour stocker les images traitées\n",
    "\n",
    "y_line_coord = int(1500 * scale_percent/100)       # Coordonnée y de la ligne de référence\n",
    "x_direction_coord = int(2000 * scale_percent/100)  # Coordonnée x du point de référence\n",
    "offset = int(8 * scale_percent/100)    # Décalage pour la détection des objets par rapport à la ligne\n",
    "in_counter = 0    # Compteur d'objets entrant\n",
    "out_counter = 0   # Compteur d'objets sortant\n",
    "\n",
    "# Informations originales de la vidéo\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "\n",
    "# Redimensionnement de la vidéo pour de meilleures performances\n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vidéo de sortie\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"mp4v\" \n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n",
    "\n",
    "# Exécution de la reconnaissance\n",
    "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # Vérification si \"frame\" est lue avec succès\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = resize_frame(frame, scale_percent)\n",
    "\n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        \n",
    "    # Utilisation du modèle YOLO pour prédire les objets dans la trame\n",
    "    y_hat = model.predict(frame, conf=0.7, classes=class_IDS, device='cpu', verbose=False)\n",
    "    # Obtention des coordonnées des bounding boxes, confiance, et classes prédites\n",
    "    boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "    conf = y_hat[0].boxes.conf.cpu().numpy()\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy() \n",
    "\n",
    "    # Affiche la forme des bounding boxes\n",
    "    print(boxes.shape)\n",
    "\n",
    "    # Mise à jour des noms des colonnes en fonction de la forme des bboxes\n",
    "    if boxes.shape[1] == 4:\n",
    "        columns = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected shape of bounding boxes data\")\n",
    "\n",
    "    # Création d'un DataFrame avec les coordonnées des bboxes\n",
    "    positions_frame = pd.DataFrame(boxes, columns=columns)\n",
    "    labels = [dict_classes[i] for i in classes]\n",
    "\n",
    "    # Dessine une ligne de comptage sur la frame\n",
    "    cv2.line(frame, (0, y_line_coord), (int(4500 * scale_percent/100 ), y_line_coord), (255,255,0), 8)\n",
    "\n",
    "    # Traitement des résultats de la détection pour chaque bboxes\n",
    "    for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        xmin, ymin, xmax, ymax = row[1].astype('int')\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5)\n",
    "        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1)\n",
    "\n",
    "        # Affiche les informations sur l'objet (étiquette et confiance)\n",
    "        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n",
    "                org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n",
    "\n",
    "        # Catégorie de l'objet pour le comptage\n",
    "        category = classes[ix] \n",
    "        # Vérifie la position de l'objet par rapport à la ligne de comptage\n",
    "        if (center_y < (y_line_coord + offset)) and (center_y > (y_line_coord - offset)):\n",
    "            if  (center_x >= 0) and (center_x <= x_direction_coord):\n",
    "                in_counter += 1\n",
    "                in_objects_counter[category] += 1\n",
    "            else:\n",
    "                out_counter += 1\n",
    "                out_objects_counter[category] += 1\n",
    "                \n",
    "    # Crée des listes pour l'affichage des compteurs           \n",
    "    in_counter_plt = [f'{dict_classes[k]}: {i}' for k, i in in_objects_counter.items()]\n",
    "    out_counter_plt = [f'{dict_classes[k]}: {i}' for k, i in out_objects_counter.items()]\n",
    "    \n",
    "    # Affiche les compteurs dans la frame vidéo\n",
    "    cv2.putText(img=frame, text='Vehicles In :', org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    cv2.putText(img=frame, text='Vehicles Out :', org= (int(2800 * scale_percent/100 ),30), \n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    xt = 40\n",
    "    for txt in range(len(in_counter_plt)):\n",
    "        xt +=30\n",
    "        cv2.putText(img=frame, text=in_counter_plt[txt], org= (30,xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                    fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "        cv2.putText(img=frame, text=out_counter_plt[txt], org= (int(2800 * scale_percent/100 ),xt),\n",
    "                    fontFace=cv2.FONT_HERSHEY_TRIPLEX,fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    cv2.putText(img=frame, text=f'In:{in_counter}', org= (int(1820 * scale_percent/100 ),y_line_coord+60),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    cv2.putText(img=frame, text=f'Out:{out_counter}', org= (int(1800 * scale_percent/100 ),y_line_coord-40),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    if verbose:\n",
    "        print(in_counter, out_counter)\n",
    "\n",
    "    frames_list.append(frame)\n",
    "    output_video.write(frame)\n",
    "\n",
    "# Ferme la vidéo   \n",
    "output_video.release()\n",
    "\n",
    "# Chemin complet vers l'exécutable ffmpeg\n",
    "ffmpeg_path = \"/opt/homebrew/bin/ffmpeg\"\n",
    "\n",
    "# Supprime le fichier de sortie s'il existe déjà\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    \n",
    "# Utilise subprocess.run pour exécuter la commande ffmpeg avec des options spécifiques\n",
    "subprocess.run([ffmpeg_path, \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-hide_banner\",\n",
    "    \"-loglevel\", \"error\", \"-vcodec\", \"libx264\", output_path])\n",
    "\n",
    "# Supprime le fichier temporaire de sortie\n",
    "os.remove(tmp_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71bdf2",
   "metadata": {},
   "source": [
    "# Sampling Transformed Frames Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [28, 29, 58]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeae9a5",
   "metadata": {},
   "source": [
    "# Executing Result Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0549c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output video result\n",
    "frac = 0.7 \n",
    "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
